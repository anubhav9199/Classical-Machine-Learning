{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Linear Regression from Scratch on Breast Cancer Data\n",
    "Linear regression is a algorithm which is starting level algorithm for machine learning prediction. This code contains only about how we can fit a linear model over user given dataset and also to get a good output result out of it. The code is fully moduler so that to keep in mind about the use of the functions in other programs also.\n",
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling Data\n",
    "Data is call for work. The Columns are selected here is according to the BREAST CANCER DATASET from WINCONSIN Hospital Easily find on Kaggle(www.kaggle.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "data = pd.read_csv('Breast_Cancer_Data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing unnecessary columns from the dataset so that you won't face any trouble regarding the dataset. I use Breast Cancer Dataset to train model and predict whether the person is having cancer or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0          1        17.99         10.38          122.80     1001.0   \n",
       "1          1        20.57         17.77          132.90     1326.0   \n",
       "2          1        19.69         21.25          130.00     1203.0   \n",
       "3          1        11.42         20.38           77.58      386.1   \n",
       "4          1        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0         0.2419  ...         25.38          17.33           184.60   \n",
       "1         0.1812  ...         24.99          23.41           158.80   \n",
       "2         0.2069  ...         23.57          25.53           152.50   \n",
       "3         0.2597  ...         14.91          26.50            98.87   \n",
       "4         0.1809  ...         22.54          16.67           152.20   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop([data.columns[0],data.columns[32]],axis=1,inplace=True)\n",
    "data['diagnosis'].replace(to_replace=['B','M'],value=[0,1],inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing of data\n",
    "The data is divided in x and y terms by taking label as y and any of the feature as x.here i use perimeter mean in the training of linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.iloc[:,1:2]\n",
    "y = data.iloc[:,0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial Regression is done by introducing higher degree terms in our regression problem to make it easier to fit a line through the data\n",
    "Here x only has one feature, therefore we will introduce higher degree terms in it for example :- \n",
    "$x^2 , x^3 , etc$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective Function\n",
    "The objective function is the Mean Square Function for the Polynomial Regression. Mean square error function shows the mean square error done by the gradient descent to lower down the error/loss in the learning and predicting of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(x,y,theta,theta_0,lam=0):\n",
    "    MSE = np.sum(np.square(x@theta + theta_0 - y) + lam*0.5*np.square(np.linalg.norm(theta)))\n",
    "    return (MSE/x.shape[0]*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theta Function\n",
    "The fdash_theta and fdash_theta0 function are the function to calculate the theta and theta0 for the gradient descent for the training of the linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fdash_theta(x,y,theta,theta_0,lam=0):\n",
    "    ans = (x@theta+theta_0-y ).T@x + lam*np.linalg.norm(theta)\n",
    "    return 2*ans.T/x.shape[0]\n",
    "\n",
    "def fdash_theta0(x,y,theta,theta_0):\n",
    "    ans = np.sum(x@theta+theta_0-y)\n",
    "    return 2*ans/x.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Creation Function\n",
    "The data creation function is use to create an augmented data of the original data. The augmented data is containing the value of the user provide data having degree 2, 3, or more. <br>\n",
    "for eg :- $x^2, x^3, etc$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_creation(x,y,degree):\n",
    "    dummy1 = x\n",
    "    dummy_temp = x\n",
    "    \n",
    "    for d in range(degree-1):\n",
    "        dummy_temp = x*dummy_temp\n",
    "        dummy1 = np.append(dummy1,dummy_temp,axis=1)\n",
    "        \n",
    "    # new data with the higher degree terms\n",
    "    augmented_data = dummy1\n",
    "    \n",
    "    # normalise data\n",
    "    xmean = np.mean(augmented_data,axis=0)\n",
    "    xstd = np.std(augmented_data,axis=0)\n",
    "    augmented_data = (augmented_data-xmean)/xstd\n",
    "    \n",
    "    ymean = np.mean(y)\n",
    "    ystd = np.std(y)\n",
    "    y = (y-ymean)/ystd\n",
    "    \n",
    "    del dummy1,dummy_temp\n",
    "    \n",
    "    return augmented_data,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotter Function\n",
    "The plotter function is use to plot the graph of polynomial regression. The graph is plot over different degree of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotter(x,y,degree,theta):\n",
    "    dummy1 = x\n",
    "    dummy_temp = x\n",
    "\n",
    "    for d in range(degree-1):\n",
    "        dummy_temp = x*dummy_temp\n",
    "        dummy1 = np.append(dummy1,dummy_temp,axis=1)\n",
    "\n",
    "    # new data with the higher degree terms\n",
    "    augmented_data = dummy1\n",
    "\n",
    "    ymean = np.mean(y)\n",
    "    ystd = np.std(y)\n",
    "    xmean = np.mean(augmented_data,axis=0)\n",
    "    xstd = np.std(augmented_data,axis=0)\n",
    "    plt.plot(x,(((augmented_data-xmean)/xstd@theta)*ystd+ymean),label='degree '+str(degree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression Function\n",
    "The Fit function is to fit the learning curve and reduce the loss of the model. The algorithm use here is\n",
    "the Stocastic Gradient Descent(SGD). The algorithm is containing the learning rate,epsilon for the stoping of\n",
    "under going algorithm to find global minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_regression(x,y,degree,alpha=0.001,eps=10e-10,lam=0):\n",
    "    \"\"\"\n",
    "    This function will fit the given data x to y\n",
    "    \n",
    "    x: array containing the data\n",
    "    y: array containing regression values\n",
    "    degree: int value stating the highest degree polynomial to be introduced\n",
    "    alpha=0.001: Learning Rate\n",
    "    eps=10e-4: tolerance \n",
    "    lam=0: value of L2 regularisation parameter\n",
    "    \"\"\"     \n",
    "    \n",
    "    augmented_data,y = data_creation(x,y,degree)\n",
    "    # Gradient Descent\n",
    "    \n",
    "    theta_i = np.zeros(shape=(augmented_data.shape[1],1))\n",
    "    theta_0i = np.zeros(1)\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    #Loop for the continuing the process again till it gets to the local minima\n",
    "    while True:\n",
    "        \n",
    "        mse_i = objective_function(augmented_data,y,theta_i,theta_0i,lam)\n",
    "        \n",
    "        history_thet = 1\n",
    "        history_thet0 = 1\n",
    "        \n",
    "        theta_f = theta_i - alpha*fdash_theta(augmented_data,y,theta_i,theta_0i,lam)\n",
    "        theta_0f = theta_0i - alpha*fdash_theta0(augmented_data,y,theta_i,theta_0i)\n",
    "        \n",
    "        mse_f = objective_function(augmented_data,y,theta_f,theta_0f,lam)\n",
    "        \n",
    "        if i%50000==0:\n",
    "            print(\"MSE at step {} is {}\".format(i,mse_f))\n",
    "        i+=1\n",
    "        \n",
    "        if abs(mse_f-mse_i)<=eps:\n",
    "            print(\"MSE at step {} is {}\".format(i,mse_f))\n",
    "            break\n",
    "                \n",
    "        theta_i = theta_f\n",
    "        theta_0i = theta_0f\n",
    "        \n",
    "    return theta_f,theta_0f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE at step 0 is 1.9832122318809255\n",
      "MSE at step 5576 is 0.9077622219819727\n"
     ]
    }
   ],
   "source": [
    "# since they are series\n",
    "x = np.array(x).reshape(569,1)\n",
    "y = np.array(y).reshape(569,1)\n",
    "theta,theta_0 = polynomial_regression(x,y,5,lam=0.05,alpha=10e-4,eps=10e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Theta\n",
    "Calculating the theta over various degree of the given data. The data is given by the user according the regression user wants. The degree is of the data which is :- <br>\n",
    "$ x^2, x^3, etc$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE at step 0 is 1.9957409436903022\n",
      "MSE at step 6127 is 0.9873917144125064\n",
      "MSE at step 0 is 1.991754316239665\n",
      "MSE at step 8968 is 0.9788381440525422\n",
      "MSE at step 0 is 1.9882738004028666\n",
      "MSE at step 7111 is 0.9734256769584445\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fde83108c10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3dd3gU1f7H8ffZTSOEZgJKTRDpVQgQOiggAuKVohQ12EC4/ES9FpQroMBFFLmoqIhKEwQUqYIgoEi9gQRpgggikNA7hBBKcn5/bAjZ7M4mJNn+fT3PPsmeOTv57mb4MJk5c0ZprRFCCOH9TO4uQAghRMGQQBdCCB8hgS6EED5CAl0IIXyEBLoQQviIAHf94IiICB0VFeWuHy+EEF4pISHhtNa6pL1lbgv0qKgo4uPj3fXjhRDCKymlDhktk0MuQgjhIyTQhRDCR0igCyGEj3DbMXR7rl+/TlJSEqmpqe4uxSuFhIRQrlw5AgMD3V2KEMINPCrQk5KSKFKkCFFRUSil3F2OV9Fac+bMGZKSkqhYsaK7yxFCuIFHHXJJTU0lPDxcwjwPlFKEh4fLXzdC+DGPCnRAwjwf5LMTwr95XKALIYRPSU+HXfPh7AGn/ygJdAdGjBjBuHHj3F0GAN999x01a9bEZDLJBVlCeIszf8G0TjDvKfjrZ6f/OAl0F0hLS8v3OmrVqsX8+fNp2bJlAVQkhHCq9HT43yT4rBkc2w4PfQTRzzj9x0qgZzN69GiqVq1K27Zt2bt3b2b7X3/9RYcOHWjQoAEtWrTgjz/+yGyPiYmhYcOGDBs2jLCwMADWrFlDmzZt6N27N7Vr1wZg5syZNGrUiHr16tG/f//MoP/pp59o0qQJ9evXp0ePHiQnJ9vUVb16dapWrersty+EyK+zB2B6Z1j+OtxVGwashwax4IJzXB41bDGrt5f8zu6jFwt0nTXKFGX4QzUNlyckJDBnzhx+++03bty4Qf369WnQoAEA/fr1Y9KkSVSuXJm4uDgGDhzIzz//zODBgxk8eDC9evVi0qRJVuvbvHkzu3btomLFiuzZs4e5c+eyYcMGAgMDGThwILNmzaJjx46MGjWKVatWUbhwYcaOHcv48eMZNmxYgb53IYSTpafDli9g1QhIuwb3vQXNXgSz62LWYwPdHdatW8cjjzxCaGgoAF26dAEgOTmZjRs30qNHj8y+V69eBWDTpk0sXLgQgN69e/PKK69k9mnUqFHmmPDVq1eTkJBAw4YNAbhy5QqlSpXif//7H7t376ZZs2YAXLt2jSZNmjj5nQohCtTZv2HRIDi0HiKqQtfJUKaey8vw2EB3tCftTPaG/qWnp1O8eHG2bdt2W+sqXLhw5vdaa2JjYxkzZoxVnyVLltCuXTtmz56dt4KFEO6Tng7xX8HK4XD9MjQeAG2HQ2Aht5Qjx9CzaNmyJQsWLODKlStcunSJJUuWAFC0aFEqVqzId999B1jCefv27QDExMTw/fffAzBnzhzDdd9///3MmzePkydPAnD27FkOHTpETEwMGzZsYP/+/QCkpKTw559/Ou09CiEKyLmDMLkVLHsFQorBEwvhwXfdFuYggW6lfv36PPbYY9SrV49u3brRokWLzGWzZs3iq6++om7dutSsWZNFixYBMGHCBMaPH0+jRo04duwYxYoVs7vuGjVqMGrUKNq3b0+dOnVo164dx44do2TJkkybNo1evXpRp04dYmJiMk+4ZrVgwQLKlSvHpk2b6NSpEw888IBzPgQhhGPp6bDlS/iwLhzfYWkbuBEqtbHbfcXvx4kaspTlu447vTSltXb6D7EnOjpaZx9PvWfPHqpXr+6WevIqJSWFQoUKoZRizpw5zJ49OzPs3cEbP0MhvMa5QzC7J5zcfattUDxEVLbpevnqDeq+/RM30i0ZO6VvNPdVuzPfJSilErTW0faWeewxdG+RkJDAoEGD0FpTvHhxpkyZ4u6ShBAFTWuInwJLX77Vdu8T0Pm/YLad3XTy2r/4z7Jbf2kvf7EF1e4q6vQyJdDzqUWLFpnH04UQPuj8YZjdG07svNXW/GXLyc9sjp6/QtN3b10RGtskkrcfruWKKgEJdCGEsE9rSJgKP7xk3f5/WyG8kk33V77bzryEpMznm4feT6kiIc6u0oqcFBVCiOzOJ8IX91mHeVQLGHbOJsx3Jl0gasjSzDAf+XBNDr7bKTPMP9n2CbWn12bJX0ucXrbsoQshxE1aw9bpsGSwdXvsEqhoPY9SWrrm4U/Ws+uI5Yr2oiEBxL3ZlkJBZgD2nt1L9yXdM/uXCSvj3NqRQBdCCIsLSTDvaUiMu9UWGAqv/gVBoVZdV/x+nP5fJ2Q+n/pUQ9pULQXA9bTrdF/SnQMXbk2Xu77neooF2x/SXJAk0B0YMWIEYWFhVpfzu8urr77KkiVLCAoKolKlSkydOpXixYu7uywhvJ/W8NvXsPj/rNsf+sgyqVYW2YciNqp4B3Oei8FkslxhPuePOYyOG53Zf+J9E2lVvpVz689CjqG7QEFMn9uuXTt27drFjh07qFKlis0UAkKIPLhwxDJfefYwf2WfTZh/sfYANYevyAzz5S+24Nv+TTCZFImXEqk9vXZmmN9f4X52PLnDpWEOEug2PHX63Pbt2xMQYPmDKiYmhqSkJJs+Qohc0hq2fg3/rQGHNtxqj/knDD8PYaUym45duELUkKWMXrYHgCdiIjn4bieq3VWUvWf30ml+JzrO75jZf2X3lUxoM8Ett4T03EMuPw6B4ztz7nc77qptmWvBgLdMnztlyhQee+yxgvlMhPA3F47Agv5wcJ11+4CNcKf1pICvzdvOt/G2QxEvXbtE09lNrfqObDaSf9zzD6eVnRueG+hu4A3T544ePZqAgAD69OlTUG9bCP+gNWz7BhYNtG6v2BL6zIOA4MymXUcu0Pnj9ZnP3+5Sk9imUaTrdGpPr2318gAVwJbHtxBgcn+cur8CIw72pJ3Jk6fPnT59Oj/88AOrV692y59zQniti0dh4UA48It1e+/voEr7zKdp6ZpHPt3AjqQLABQJDmDzUMtQxHFbxjF993Srly96eBF3F7/b6eXnlhxDz8KTp89dvnw5Y8eOZfHixZl/QQghcnBzr3x8deswDyoCr/1tFeY//X6cSm8uywzzqX0bsvPtBzh77Ti1p9e2CvOnaz3NztidHhXm4Ml76G6QdfrcyMhIm+lzBwwYwKhRo7h+/To9e/akbt26TJgwgccff5wPPviATp065Wr63PT0dAIDA/nkk0+IiYnJnD735mGcUaNGUaVKFavXDxo0iKtXr9KuXTvA8h9J9mP2QogsLh6zjF7Zv9K6vcNYaNw/8x6fl6/e4N53VnItLR2AhlElmNuvCUphc3gFYHOfzRQKcN+c547I9Ln5JNPnCuFhtIYdcy0nPrN74Te449Ze9ZfrDjBq6Z7M5z8ObkH10kX5Zs83jNlsfXj06we/pl4p199WLjuZPteJZPpcITzIpeOw+AXYt8K6PfppePC9zKluj124QpMxt2ZFfDymAqP+UZtzqeds9srblG/Dh20+9IrzVhLo+STT5wrhAbSGnd/B/OfsL+/838xvbYYivnk/JYsE0+SbJiRft74GxFWX7BcUCXQhhHe7dAJ+eBH2LrNdVqEpPGVpzz4UccRDNejbrCKrDq3ipQXWU+R+fN/HtC7f2plVO4UEuhDCO2kNO+fB/GftL+/3K5SpZxmK+Mn6zNErhYPMxP+7HWlcsTm8UqZwGX7s9iMm5Z0DACXQhRDeJ/kkLHkR9i61XVYiCl7YBkqxcvcJnptxa/DFlL7RtKlaiid/fJJtp6yvK1nZfSV3Fb7LyYU7V46BrpSaAnQGTmqtbe6lpJRqDSwC/s5omq+1fqcgixRCCMCyV77re/j+GfvLn14BFWJshiI2iCzBd/2bsPVkAnVmtLV6ydDGQ+lZraezK3eJ3OyhTwMmAjMc9Fmnte5cIBV5EE+aPvett95i0aJFmEwmSpUqxbRp0yhTxvkT5gvhMZJPWu4g9McP9pcPOwcmk81QxGUvtCCqZAB1v65j85Ktj28l0M5Nnr1VjoGutV6rlIpyfim+Ky0tDbPZnK91vPrqq4wcORKAjz76iHfeeUcuLBL+Y9d8mPeU/WX3tIXe33H80jVixqzObO7TuAJPtgri3xue4Y+zf1i9ZOHDC6lU3Pa+oN6uoI78N1FKbVdK/aiUqmnUSSnVTykVr5SKP3XqVAH96ILlqdPnFi1aNPP7y5cve8WYWCHyLfkUzH3COMwHxsHj3zNkwS6rMF//enMWX+xN9yXdrcL8iRpPsDN2p0+GORTMSdGtQKTWOlkp1RFYCFS211FrPRmYDJYrRR2tdOzmsTb/q+ZXtTuq8Xqj1w2Xe/r0uUOHDmXGjBkUK1aMX375xWa5ED7l9wXwXV/7y0pEwaAEdh2/TOcht06MDn+oBjeKrOLBxc1tXhLXO47QQN+eBynfga61vpjl+2VKqU+VUhFa69P5Xberefr0uaNHj2b06NGMGTOGiRMn8vbbbxfk2xfCM1w+DUtfht0GU2g8/RNp5RrR9bONbE88D1iGIi56sTqPLLE9lTfjwRncW+peZ1bsMfId6Eqpu4ATWmutlGqE5TDOmfyu19GetDN58vS5N/Xu3ZtOnTpJoAvf8/tC+C7WePnQ46zad5Fn37x1EdFXsQ14f3dfHllyxKprTOkYPm/3udeOKc+LHN+pUmo2sAmoqpRKUko9o5R6Xin1fEaX7sAupdR24COgp3bXjF/55MnT5+7bty/z+8WLF1OtWrUCeMdCeIjLZyyHV4zCvNmLpLx5hqpvr+HZjHHlDSJLUK7uGF7c3I4jydZhvvaxtXzR/gu/CnPI3SiXXjksn4hlWKPX8+Tpc4cMGcLevXsxmUxERkbKCBfhO3Yvgm+fNF7eZSJfpTRn5LBbE279p3cgY37rD9esu45vPZ52ke2cVKjnk+lz80mmzxUij1LOwrJXLBcKGTgxOInGY9dmPn+s0V0su9TXpl+x4GKseXSNR9wGztlk+lwnkulzhciDPUtg7uPGy3tM5429dzM7S5g/1HY1y46stOm6otsKyoTJRXYggZ5vMn2uELch5Sz8+JplqlsDvz93kE4fbwQSARjUvijTEweyxvowOf9q8C/61urrvFq9kAS6EMI1/lgKc3o77NK15A9s/XgjAIWCrxJw93CmJ9r28+TbwLmTBLoQwrlSzsKPr8PObw27bG09g67LAyDRcllLsRpDSddpNv3mPTSPqndUdVqp3k4CXQjhPH8sgzkOB8pRM202l5dbBmdUjTrB0UL/JT3bWI2ulbsyoskImfIiBxLoQoiCd+WcZa98x1zDLivrfsRzcRGARpkvE1ZlJEft9NvQawNFg4raWSKy869R9wWoY8eOnD9/3mGfYcOGsWrVqjytf82aNXTu7HMzEgt/sHc5jK3oMMwrp87ICHMoUn0IYVVG2vT5ov0X7IzdKWF+G2QP/TZprdFas2yZnfsXZvPOO3KfD+FHrpyD5W/AduNpLOaWf4vX91mukzCH7Sa0vO1tFmqE1+Cbjt9gNuVvyml/5N176LNmQVQUmEyWr7NmFchqx48fT61atahVqxYTJkzg4MGDVK9enYEDB1K/fn0SExOJiori9GnL/GMjR46kWrVqtGvXjl69ejFu3DgA+vbty7x58wCIiopi+PDh1K9fn9q1a2dOv7t582aaNm3KvffeS9OmTa2m7BXCa/y5At6r5DDMq6ZOywjzNIpUH2I3zH/u8TNzO8+VMM8j791DnzUL+vWDlBTL80OHLM8B+vTJ82oTEhKYOnUqcXFxaK1p3LgxrVq1Yu/evUydOpVPP/3Uqn98fDzff/+93Sl3s4uIiGDr1q18+umnjBs3ji+//JJq1aqxdu1aAgICWLVqFW+++Wbm3DBCeLwr52HFm7DNeGfqgipG/SsTScNM4cjPMYX+bdNnZLOR/OOefzizUr/gvYE+dOitML8pJcXSno9AX79+PY888kjmTIldu3Zl3bp1REZGEhMTY7f/ww8/TKFCljGxDz30kOG6u3btCkCDBg2YP38+ABcuXCA2NpZ9+/ahlOL69et5rl0Il9q3Emb3gnTjbfahq6PYqe9GBZ6hyD3v2+0T/3g8weZgZ1XpV7w30A8fvr32XDKa2ybrVLi56W9PcLBlozWbzdy4cQOw3Cu0TZs2LFiwgIMHD9K6devbK1gIV0u9AMvfhG0zHXarmzqZCyqIItWH2F3+wyM/EFk00hkV+i3vPYZeocLttedSy5YtWbhwISkpKVy+fJkFCxZYzbqYXfPmzVmyZAmpqakkJyezdOlSw772XLhwgbJlywIwbdq0/JQuhPPtXwXvV84xzKNSv+HGPR9RpJrtnbeeq/0cO57cIWHuBN67hz56tPUxdIDQUEt7PtSvX5++ffvSqFEjAJ599llKlChh2L9hw4Z06dKFunXrEhkZSXR0tOEUuva89tprxMbGMn78eO6777581S6E06RegBVD4bevHXZ76doAloZfoUhF+3vlm3ptIiwozBkVCrx9+txZsyzHzA8ftuyZjx6dr+PneZWcnExYWBgpKSm0bNmSyZMnU79+fZfXATJ9rnCC/athTh+4ccVht6irUylSbbjdZTM7zqRuybrOqM7v+O70uX36uCXAs+vXrx+7d+8mNTWV2NhYt4W5EAUq9SL8NBS22g4vzGrQtf/j16qLKIJtmLcp34YJbSb43Z2D3MW7A91DfPPNN+4uQYiC9dfPlr3y6ykOu1UOGUxIxQV2l615dA3hhcKdUZ0w4HGBrrWWCXjyyEtv5So8SepFWPkWJExz2O35ay+woepCQrAN8wmtJ3B/5P1OKlA44lGBHhISwpkzZwgPD5dQv01aa86cOUNISIi7SxHe6q9fLHcRupbssFvtihWAhTbtpQuXZukjSwk0BzqpQJETjwr0cuXKkZSUxKlTp9xdilcKCQmhXLly7i5DeJurl2DlMIh3fPvEAelPsb7SarvLfuz6I+WKyLbnbh4V6IGBgVSsWNHdZQjhPw78atkrv3rRYTfLXrltmL8S/QpP1nhS/qL2EB4V6EIIF7manLFX/pXDbh+WKMaXxe1fVxHXO47QwFBnVCfySAJdCH/z91rLXnnqBcMuV5SiUVR5u8u+7fwt1cPlWgdPJIEuhL+4mgyrhsOWLx12sxxesdWtcjeGNRkmY8o9mAS6EP7g4HrLXvmVc4Zdvi5ahPfC7U9zse6xdRQPKe6s6kQBkUAXwpdduwyrRsDmyYZd0oG6Bnvln7X9jOZlmzunNlHgJNCF8FUHN2TslZ817GJ0eKV2RG2mPzidQJOMKfcmEuhC+Jprl2H1OxA3ybDL7qBAHitb2u6yld1Xclfhu5xVnXAiCXQhfMmhjTD3CUg5bdjFaK98WJNh9KjSw1mVCReQQBfCF1xLydgr/8ywi1GQKxRxfeIoFFDIWdUJF5FAF8LbHdoE3z4Jl0/aXXzBZKJ5pP3L8ud3mU/lEpWdWZ1wIQl0IbzVtRT4eRT87xPDLkZ75X1r9uXlBi/LJfs+RgJdCG90OM6yV5583O7iQXeW5NdQ+4dQ1vdcT7Hg3N8mUXgPCXQhvMn1K5a98k0T7S7WQB2DvfIv239J49KNnViccDcJdCG8ReJmy175pWN2FxsdXmletjkT75uI2WR2ZnXCA+Q4KYNSaopS6qRSapfBcqWU+kgptV8ptUMpJTfULAizZkFUFCgFAQGWr1FRlvaby0ymW20DB4LZbOmnFISFWdrtre/mw2SCkBDrtpvrMJms23P7yOvrnPGwV8vNz9JbHiEhEKigfQh81c5umMeFBBuG+eoX/+CzdpMwFwqFiAjr33HWR0SEZRu6uV1FRFgeWbex3G6vWR/Bwblbj71t2lF7Tq/LzWvzIus6c/sZOaMOI1prhw+gJVAf2GWwvCPwI6CAGCAup3VqrWnQoIEWBmbO1Do0VGuwfQQFaR0YaN1mMtnvazJZ1uVoffLw7EdZs9Yvhmk9vKjdR61ptew+FjUtXrB1hIZatqPb3V5zsx57rw8N1XrAAPvtN19v9Dqjbd7Re8jvv8vbfW/5qAOI19p+rirLcseUUlHAD1rrWnaWfQ6s0VrPzni+F2ittbb/d2GG6OhoHR8fn+v/ePxKVBQcOlQw64qMtHwtqPUJ1zADbYKhWbDdxUZ75BHnr7P81T8Jvp7zv+vbFhkJBw/att/u9pp9PUavN5shLc349Uavc7TNG72H3MjN+8zte8tHHUqpBK11tL1lBXEMvSyQmOV5UkabTaArpfoB/QAqVLC/QQrg8GHPXJdwjTImeDQUitkeEU1RisYG85QveuNP7j52zXl1GW1Lt7uNZe9v9Hp7YZ61f17qyc+/h9y8NrfvzUn/LgtiYmNlp83u7oHWerLWOlprHV2yZMkC+NE+qiD/s6tQoWDXJ5zHDNwfDM+F2Q3z2hUr2A3z5xeeZEffXc4NczDejm53+8re3+j1ZoOTuDf7O6qnoGq93dfm9r056d9kQQR6EpB1KysHHC2A9fqv0aMh1ODWXkFBEJhtBjyTwa/RZLKsy9H6hGcoY4LBYdDc9hBL2/JlDA+xbHp+N/9ceNLuXlWBCg21bEf23M72ZW899l4fGgr9+tlvv/l6o9cZbfOO3kNu5PQ+b+e95acOR4wOrmd9AFEYnxTthPVJ0c25WaecFM3BzJlaR0ZaTqKYzZavkZG3TvhERmqt1K22AQOsT44WLmx94iXr+m4+lNI6ONi67eY6lMrbybO8vs4ZD3u13PwsPeVhRuv7g2/7pGd8FYOTc0YnyIOCtA4PN+4THm7Zhm5uV+HhlkfWbSy326u9n5vTeuxt047ac3pdbl6bF1nXmdvPqIDrID8nRZVSs4HWQARwAhgOBGb8ZzBJWa4dngh0AFKAp7TWOZ7tlJOiwu8dSYCZ3e3OV260R94hqgNjW46V28D5sXydFNVa98phuQb+mcfahPA/N67Cr2Nh3Qc2i44GmHmgfFm7L1vz6BrCC4U7uzrhxeRKUSFc6ehvMK0zXEu2WWS0V/5ey/d4sOKDzq5M+AAJdCFc4ca1jL3ycTaLjII8qmgU87vMJ9Ast4ETuSOBLoSzHd0Gk1uTfTSvxngiraWPLKVCURluKm6PBLoQznLjGqwZA+vH2ywy2isfXH8wz9R6RuYpF3kigS6EMxzbDp+3tGneGRRE77L2b8Ac1zuO0EC5XkDknQS6EAXpxjX4+R3Y+LHNIqO98lkdZ1GnZB1nVyb8gAS6EAXl+E6Y1Nym2SjIu1TqwshmI2VMuSgwEuhC5FfadVgxFDZ/btXs6KTn2sfWUiKkhAuKE/5EAl2I/Di+CyY1s2k22iv/b+v/0jayrbOrEn5KAl2IvEi7Dktfhq0zrJo3Fgqh/12lbLpXKVGFOZ3nEGiSMeXCeSTQhbhdx3bA5y1smo32ypd3W07ZMPuX8wtRkCTQhcittOswvx/8Pt+q2SjIX4l+hdiasa6oTAhAAl2I3Dn6W8bVnrekAfUMwnxzn80UCijk/LqEyEICXQhH0m7A7J6wf6VVs9Fe+ZzOc6gZXtMVlQlhQwJdCCOJm+GrdlZNywuH8mqpCJuu3Sp3Y3iT4XLJvnArCXQhsku7AVMegCO3bsDiaEz5+p7rKRZczEXFCWFMAl2IrA78CjO6WDUZHV6ZeN9EWpVv5YqqhMgVCXQhwDKC5ZNGcPZAZtN1oL6dMK8WcidzeqzAbDK4K70QbiKBLsQfy2CO9Z0WjfbKV3ZfyV2F7c+WKIS7SaAL/3U9FcZXgyvnMpvmFgljVMQdNl3fqPYkvRu/6srqhLhtEujCP+34FuY/l/nU0UnP+MfjCTYHu6gwIfJOAl34l9SL8G55qyajwyvzOnxN1TvruaIqIQqETMQs/Efc51ZhnqKU3TB/7OIldjy5Q8JceB3ZQxe+L/kUjLvHqslor3zTYxsICynqiqqEKHCyhy58l9bwyxirMP+0eDG7YT7p+El2xu6UMBdeTfbQhW86dxA+rJv51GgirYrXrrPgmd9lTLnwCRLowrekp8HyIbB5cmaT0eGV1ZcLUWrgTldVJoTTSaAL35HtJs3nTSZaRJaz6fbW6bM8+q8jrqxMCJeQQBfe73oqLOgPuxcCjseUb23+EYGV2riwOCFcRwJdeLeD62Fap8ynwyLuYEGRMJtuC5OOUmnoGVdWJoTLSaAL75R6AWb1gMQ4wHgire4XLzHs+T9QgSEuLlAI15NAF95n92L49onMp0YnPeMOJhI6/LyrqhLC7STQhfe4eMxyX8/k4wAcM5tpX6GsTbcvj52g8ZCTLi5OCPeTQBeeLz0dtk6DH14CHJ/03H7ehEnCXPgpCXTh2U7vh4kNMp/2vasUCYVsj4f/ciiJiGHnbNqF8Cdy6b/wTGnXLZftZ4R5asZEWtnDfNjpM+xs8LaEuRDkcg9dKdUB+BAwA19qrd/Ntrwv8D5w82qNiVrrLwuwTuFPjiTAF/dlPjU66bn178MEjrjgqqqE8Hg5BrpSygx8ArQDkoAtSqnFWuvd2brO1VoPckKNwl9cuwzLXoNtMwHYGxhI93KlbbotTjpKxTdOgsy/IoSV3OyhNwL2a60PACil5gAPA9kDXYi8278KZnYDjCfSqpd6la+PnQDZKxfCrtwEelkgMcvzJKCxnX7dlFItgT+Bl7TWidk7KKX6Af0AKlSw/2e08DOXz8C3T8Kh9QA0rVCOS2bbUzubDyZSSMaUC+FQbk6KKjttOtvzJUCU1roOsAqYbm9FWuvJWutorXV0yZIlb69S4Vu0hm2z4f274dB6LposJz2zh/mUYyfYWXWghLkQuZCbPfQkIOtNGMsBR7N20FpnnSTjC2Bs/ksTPuvcIfi8JaSedzimfMffh1FyeEWIXMtNoG8BKiulKmIZxdIT6J21g1KqtNb6WMbTLsCeAq1S+Ib0NNgwAVa/A8D/QoJ5rvSdNt3WHkqixNBTYJbLJIS4HTn+i9Fa31BKDQJWYBm2OEVr/btS6h0gXmu9GHhBKdUFuAGcBfo6sWbhjY7vgknNAOOJtHpcvMSwM+fkpKcQeaS0zn443DWio6N1fHy8W362cBkMaCwAAA2tSURBVKHrqbD8dUiYBhiPKf/t78MESJALkSOlVILWOtreMvmbVjhPlrnKT5tNtKlge/egpYlHqXDvUxA7ztXVCeFzJNBFwbtyHmZ1h6QtDk967vz7sBxeEaIASaCLgrVrPsx7CoAlYaG8WTLCpsuWg4mEDDsHyt6IWCFEXkmgi4Jx8RiMrwZYJtJqGFXepstz5y/wwrkLslcuhJNIoIv8SU/PGIr4NmB80lPGlAvhfBLoIu9O74OJlpPtiQEBdCxfxqbL+kNJFHtuDZSu6+LihPA/Euji9t24BvOfg90LDSfSAjnpKYSrSaCL23M4Dqa0B2BS8aJ8UqK4TZdtfx/GLEEuhMtJoIvcuZoMH9WDy6e4pBRN7Zz0fOP0WXpfSpa9ciHcRAJd5Gz7XFjQT8aUC+HhJNCFscun4f1KAOwOCuSxsrZ3D0r4+zBBw8/LmHIhPIAEurClNSwfAnGTDCfSAtkrF8LTSKALa1mGIr5aMpzlYYVtusiYciE8kwS6sEhPg8+awak9nDGZaB1pO5HW5GMnaPJKEgQEuaFAIUROJNAF/PkTfNNDTnoK4eUk0P3ZtcvwH8vVnRsLhdD/rlI2Xbb/fRiTBLkQXkEC3V+teRfWjOGqgugog73ybj9DmNzMWwhvIYHub84dgg/rANChXBmOBNpuAnJ4RQjvJIHuL7SGKQ9AYhxHAsx0KF/WpsvyxCOU/fdZNxQnhCgIEuj+4MAamPEw6UBdo5OeDy+D4raX8wshvIcEui/LctJzUVhh/l0y3KaLHF4RwndIoPuqdeNh9dskK0UTOxNpAeyM3eniooQQziSB7mvO/m2ZFREHdw96dC2qUAlXViWEcAEJdF+Rdh1mdYcDa/gzMJBu5Wwn0pJ5yoXwbRLovmD/KpjZzfFEWnJ4RQifJ4HuzS6fgffvBuCjEsX4ongxmy5bem0lJCjQ1ZUJIdxAAt0bpafD+g/g51GcNZloZWcirQ7Jl3n/nwfcUJwQwl0k0L3N8V0wqZnDibS2PbEDs0luOCGEv5FA9xZXk2FOL/h7LVtCgnm69J02XZY/vJmyxQu5oTghhCeQQPd0WsPuhfBdX8tEWkZDEZ/cgZLbwAnh1yTQPVmWMeW9S9/JzpBgmy7reiRQPFRuOCGEkED3TDeuwi//gQ0TOGY2076C7URa/41ZQtuqUa6vTQjhsSTQPc2BX2FGF4cTaW3pvY2QQLNr6xJCeDwJdE+RfBK+7gondrI4rDBD7Uyk9W37DVQvXdQNxQkhvIEEurulp8GWL+HH17isFDF29spbRzzHhAf/T4YiCiEckkB3p6O/weTWgPFEWjIUUQiRW6bcdFJKdVBK7VVK7VdKDbGzPFgpNTdjeZxSKqqgC7UxaxZERYHJZPk6a5Zxv7AwUMryMJth4MBbywcOhIAAy7KAAMvzmjVv9XfGI0RBt1CY3Jq9QYF2w/w/75ZgR99dlC0R6txaPPUREWH9O7X3e8renvW1ERHW20Zut5e8bmdCeACltXbcQSkz8CfQDkgCtgC9tNa7s/QZCNTRWj+vlOoJPKK1fszReqOjo3V8fHzeqp41C/r1g5SUW22hoTB5MvTpY90vNhbS0mzXMWCA5etnn+WthryqHQBdQx1OpLXu+YMUT012bV2eKDAQpk6FDRvs/55q1IDdu23bswsKsoznv379Vpu97SW73G5nQriQUipBax1td1kuAr0JMEJr/UDG8zcAtNZjsvRZkdFnk1IqADgOlNQOVp6vQI+KgkOHbNsjI+HgwZz7gWVPHeyHvTOEm+D5whCgeK1kOD+GFbbp8vkwRdPDMiuilchISEpyzu8p+/aSXW63MyFcyFGg5+YYelkgMcvzJKCxUR+t9Q2l1AUgHDidrZB+QD+AChXs753myuHDuWs36geuC/IAoG0wNA7mtNlEmwq2E2k9PuVeBm+cS8iNa66pyZscPmzZu3bWuvOyPKfXCeEmuQl0e0Mrsv8Ly00ftNaTgclg2UPPxc+2r0IF+3tO2f+TMOoHrtlDvycA+oQ6nEjr21eTqX7qa+fV4O0qVHDeHnpOOxW53c6E8BC5OSmaBGS9KWU54KhRn4xDLsWAswVRoF2jR1uOZWYVGmppz97PbHABTr9+loczFFHwUhj0CWVFaCG7Yf7Sx3ez7andVD910Dk1+ILAQMvv0Oj3VKNG7tYTFGRZV1b2tpfscrudCeEptNYOH1j24g8AFYEgYDtQM1uffwKTMr7vCXyb03obNGig82XmTK0jI7VWyvJ15kzjfoULa235w11rk0nrAQNuLR8wQGuz2bLMbLY8r1HjVv/beSi0bhWs9fCi+sqIYrrWtFo2j8d7vqkTi5bM2/r96REebv07tfd7yt6e9bXh4dbbRm63l7xuZ0K4CBCvDXI1x5OiAEqpjsAEwAxM0VqPVkq9k7HixUqpEOBr4F4se+Y9tdYO766Qr5OinihxM3zVDjAeUz663goeqlNaZkUUQuRZfk+KorVeBizL1jYsy/epQI/8FOm1Us7C5y3hQiJ7gwLpXtb25swdQmcytFN1mRVRCOFUcqVoXmkNGz+GlW8ZTqRV9Oy/eP+hzjS9J8L19Qkh/I4Eel6c+B0+awpAbOlSbA0JsenyxJ3f8ULvyjIrohDCZSTQb8fVZPigKlxL5pTZxH12xpTffekz3u1WR2ZFFEK4nAR6bmQ5vAL2T3qmHenPKy07Eds0SmZFFEK4hQR6Tk7vg4mWE8pfFy3Ce+ElbLpEp3/FyAG1KFci1GaZEEK4igS6kRvXYFRJAFKUonFUeZsuQYfHM+yhGnSpW0aGIgoh3E4C3Z6Vw2DDh4D9wysph57jkeqtGPpydUoUlqGIQgjPIIGe1bEd8HkLADaGhNC/dCmbLnec/IjPe9eWoYhCCI8jgQ5wPRVG32n5FvvzlKfsHUu/lnczWIYiCiE8lAT6lAfh8EYA2pYvw4kA64/k8l8vUbtUFcYMqkONMjIUUQjhufw30PcsgbmPA7AvMJCu5awv2U8/2Q19KYah7avSV4YiCiG8gP8F+qUT8EEVANKAenYOr1za8y6tq5Zk1LMyFFEI4T38J9DTbsDI8MynY+8ozsxi1odQruwbQbHgonzYU4YiCiG8j38E+voJsGo4AMfNZtpVKGu1uNiZN0k6WZTuDcoxtKMMRRRCeCffDvTDcTClPYDd28DdaW7I/l3duCM8lFnP1qaZDEUUQngx3wz0y6fh/UqZT1eHFuLFO0tadQlKfJ+/U9J5vtXdvNhWhiIKIbyfbwV6ehp88yjsXwXAeZOJFpHWMyJW00PY8kdx6pQLY8bTtalZppg7KhVCiALnO4G+6VNY8QZgObzydp22fH/pz8zFRc68zNGTpdgC/LtTdZ5qVlGGIgohfIr3B3riFviqbebTzVENeUadgIwwv+/OJ1i0piaXMpZ/P6ApDSJtZ0wUQghv592BfuL3zDBPVorWd1fiavoJAEoE38Hh7S+yaI9lxErb6qX44sloGYoohPBZ3h3ooRFQ6X7Gl4lkatIqSL8GQOeI/zB7nSmz28//asXdJcPcVaUQQriEVwd6SkgRGqfvg6R9ADxcsSczl9Vjdsbyga0r8VqHau4rUAghXMirA/3Pc5bj5AGmABroj5i57GLmst/eaicXCAkh/IpXB3q9UvWY2upXuk/axCosYT6uR126N7C9ebMQQvg6rw70P09covukTQCULV6In19pRXCAXCAkhPBPXh3oxQoF0ijqDl5qV4UmlcJzfoEQQvgwrw70O4uG8O3zTdxdhhBCeARTzl2EEEJ4Awl0IYTwERLoQgjhIyTQhRDCR0igCyGEj5BAF0IIHyGBLoQQPkICXQghfITSWrvnByt1Cjjklh/uPhHAaXcX4aHks7FPPhdj/vrZRGqtS9pb4LZA90dKqXitdbS76/BE8tnYJ5+LMflsbMkhFyGE8BES6EII4SMk0F1rsrsL8GDy2dgnn4sx+WyykWPoQgjhI2QPXQghfIQEuhBC+AgJdCdRSk1RSp1USu3K0naHUmqlUmpfxtcS7qzRHQw+lxFKqSNKqW0Zj47urNFdlFLllVK/KKX2KKV+V0oNzmj36+3Gweci2002cgzdSZRSLYFkYIbWulZG23vAWa31u0qpIUAJrfXr7qzT1Qw+lxFAstZ6nDtrczelVGmgtNZ6q1KqCJAA/APoix9vNw4+l0eR7caK7KE7idZ6LXA2W/PDwPSM76dj2Sj9isHnIgCt9TGt9daM7y8Be4Cy+Pl24+BzEdlIoLvWnVrrY2DZSIFSbq7HkwxSSu3IOCTjV4cU7FFKRQH3AnHIdpMp2+cCst1YkUAXnuAzoBJQDzgGfODectxLKRUGfA+8qLW+6O56PIWdz0W2m2wk0F3rRMbxwJvHBU+6uR6PoLU+obVO01qnA18Ajdxdk7sopQKxhNYsrfX8jGa/327sfS6y3diSQHetxUBsxvexwCI31uIxboZVhkeAXUZ9fZlSSgFfAXu01uOzLPLr7cboc5HtxpaMcnESpdRsoDWWKT5PAMOBhcC3QAXgMNBDa+1XJwgNPpfWWP5s1sBBoP/NY8b+RCnVHFgH7ATSM5rfxHK82G+3GwefSy9ku7EigS6EED5CDrkIIYSPkEAXQggfIYEuhBA+QgJdCCF8hAS6EEL4CAl0IYTwERLoQgjhI/4fIeNNpARYaCEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for d in range(1,4):\n",
    "    theta,theta_0 = polynomial_regression(x,y,d,lam=0.1,alpha=10e-4,eps=10e-10)\n",
    "    plotter(x,y,d,theta)\n",
    "plt.scatter(x,y,label='original',color='red')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Linear Regression\n",
    "In case of Linear Regression. The linear model gives theta from the given formula.- <br>\n",
    "$\\theta = (X^{T}X)^{-1}X^{T}Y$ <br>\n",
    "This formula is calculating the weight from simple linear regression model. Simple Linear regression is a algorithm which gives an overview of the theta over single feature of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple Linear Regression\n",
    "\"\"\"Simple Linear Regression function is a function use to calculate theta(weight) over a particular feature \n",
    "given by the user. Here a concept of polynomial regression of creating augmented data is used and the theta\n",
    "is calculated over the augmented data.\"\"\"\n",
    "def find_theta(x,y,degree):\n",
    "    dummy1 = x\n",
    "    dummy_temp = x\n",
    "\n",
    "    for d in range(degree-1):\n",
    "        dummy_temp = x*dummy_temp\n",
    "        dummy1 = np.append(dummy1,dummy_temp,axis=1)\n",
    "\n",
    "    # new data with the higher degree terms\n",
    "    augmented_data = dummy1\n",
    "\n",
    "    ymean = np.mean(y)\n",
    "    ystd = np.std(y)\n",
    "    xmean = np.mean(augmented_data,axis=0)\n",
    "    xstd = np.std(augmented_data,axis=0)\n",
    "\n",
    "    adj_data = (augmented_data-xmean)/xstd\n",
    "    adj_labels = (y-ymean)/ystd\n",
    "\n",
    "    theta = np.linalg.inv(adj_data.T @ adj_data)@adj_data.T@adj_labels\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Theta Calculate Simple :  [[   7.03664512]\n",
      " [ -52.78471227]\n",
      " [ 123.26784157]\n",
      " [-115.02924456]\n",
      " [  38.20976639]]\n"
     ]
    }
   ],
   "source": [
    "thet = find_theta(x,y,degree = 5)\n",
    "print(\"The Theta Calculate Simple : \",thet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
